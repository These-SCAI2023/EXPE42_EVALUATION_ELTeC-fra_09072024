{
 "cells": [
  {
   "cell_type": "code",
   "id": "e4672dfe-0b23-44d7-9663-f36e99f983a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:29:26.910175Z",
     "start_time": "2024-10-26T07:29:26.589854Z"
    }
   },
   "source": [
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "path_data = \"../ANNOTATION_TRANSFORMED_AIA/DAUDET/*\"\n",
    "liste_annote=[]\n",
    "for annotateur in glob.glob(path_data):\n",
    "    for file in glob.glob(\"%s/*\"%annotateur):\n",
    "        annotation = pd.read_csv(file, on_bad_lines='skip')\n",
    "        if \"Kraken\" in file:\n",
    "            annotation.columns = [\"Label\"]\n",
    "            liste_annote.append([annotation])\n",
    "\n",
    "    \n",
    "   \n",
    "   "
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "5ec0b090-9435-49b8-a66c-3cb48413e720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T06:04:28.506338Z",
     "start_time": "2024-10-26T06:04:26.075685Z"
    }
   },
   "source": [
    "!pip install nltk"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/caroline/Documents/EXPE_2024/EXPE42_EVALUATION_ELTeC-fra_09072024/.venv/lib/python3.10/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: click in /home/caroline/Documents/EXPE_2024/EXPE42_EVALUATION_ELTeC-fra_09072024/.venv/lib/python3.10/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /home/caroline/Documents/EXPE_2024/EXPE42_EVALUATION_ELTeC-fra_09072024/.venv/lib/python3.10/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/caroline/Documents/EXPE_2024/EXPE42_EVALUATION_ELTeC-fra_09072024/.venv/lib64/python3.10/site-packages (from nltk) (2024.9.11)\r\n",
      "Requirement already satisfied: tqdm in /home/caroline/Documents/EXPE_2024/EXPE42_EVALUATION_ELTeC-fra_09072024/.venv/lib/python3.10/site-packages (from nltk) (4.66.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "258fa81c-0205-414f-b755-f92ec8ac9f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:58:42.835341Z",
     "start_time": "2024-10-26T07:58:42.690437Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from  nltk.metrics import agreement\n",
    "liste_version = [\"Kraken-base\",\"REF\",\"TesseractFra-PNG\"]\n",
    "version = liste_version[0]\n",
    "liste_auteur = [\"CARRAUD\",\"DAUDET\"]\n",
    "auteur = liste_auteur[1]\n",
    "# Assume this CSV has the format <tweet_id>,<label>\n",
    "anno1_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/CF_{auteur}/CF_DAUDET_petit-chose_{version}_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\", on_bad_lines='skip')\n",
    "# anno1_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/CF_CARRAUD/CF_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\", on_bad_lines='skip')\n",
    "# anno1_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/CF_CARRAUD/CF_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\", on_bad_lines='skip')\n",
    "anno1_df.columns = [\"Token_1\",\"Label_1\"]\n",
    "# print(\"pendant\",anno1_df)\n",
    "anno1_df=anno1_df.iloc[:6500]\n",
    "print(\"pendant anno1\",anno1_df)\n",
    "\n",
    "\n",
    "# Assume this CSV has the format <tweet_id>,<label>\n",
    "anno2_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/LM_{auteur}/LM_DAUDET_petit-chose_{version}_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno2_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/LM_CARRAUD/LM_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno2_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/LM_CARRAUD/LM_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "\n",
    "anno2_df.columns = [\"Token_2\",\"Label_2\"]\n",
    "anno2_df=anno2_df.iloc[:6500]\n",
    "print(\"pendant anno2\",anno2_df)\n",
    "\n",
    "# Assume this CSV has the format <tweet_id>,<label>\n",
    "anno3_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/MB_{auteur}/MB_DAUDET_petit-chose_{version}_spaCy.3.7.3-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno3_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/MB_CARRAUD/MB_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno3_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/MB_CARRAUD/MB_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "anno3_df.columns = [\"Token_3\",\"Label_3\"]\n",
    "anno3_df=anno3_df.iloc[:6500]\n",
    "print(\"pendant anno3\",anno3_df)\n",
    "\n",
    "# Assume this CSV has the format <tweet_id>,<label>\n",
    "anno4_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/AA_{auteur}/AA_DAUDET_petit-chose_{version}_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno4_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/AA_CARRAUD/AA_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno4_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/AA _CARRAUD/AA_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "anno4_df.columns = [\"Token_4\",\"Label_4\"]\n",
    "anno4_df=anno4_df.iloc[:6500]\n",
    "print(\"pendant anno4\",anno4_df)\n",
    "\n",
    "# # Assume this CSV has the format <tweet_id>,<label>\n",
    "# anno5_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/AB_{auteur}/AB_DAUDET_petit-chose_{version}_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# # anno3_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/AB_CARRAUD/AB_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# # anno5_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/AB_CARRAUD/AB_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno5_df.columns = [\"Token_5\",\"Label_5\"]\n",
    "# anno5_df=anno5_df.iloc[:6500]\n",
    "# print(\"pendant anno5\",anno5_df)\n",
    "\n",
    "# Assume this CSV has the format <tweet_id>,<label>\n",
    "anno6_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/CKP_{auteur}/CKP_DAUDET_petit-chose_{version}_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno6_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/CKP_CARRAUD/CKP_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno6_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/CKP_CARRAUD/CKP_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv\",sep=\";\",on_bad_lines='skip')\n",
    "anno6_df.columns = [\"Token_6\",\"Label_6\"]\n",
    "anno6_df=anno6_df.iloc[:6500]\n",
    "print(\"pendant anno6\",anno6_df)\n",
    "\n",
    "# Assume this CSV has the format <tweet_id>,<label>\n",
    "anno7_df = pd.read_csv(f\"../ANNOTATION_TRANSFORMED_AIA/{auteur}/DN_{auteur}/DN_DAUDET_petit-chose_{version}_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno7_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA_ARCHI/CARRAUD/DN_CARRAUD/DN_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg.csv.aia.csv\",sep=\";\",on_bad_lines='skip')\n",
    "# anno7_df = pd.read_csv(\"../ANNOTATION_TRANSFORMED_AIA/CARRAUD/DN_CARRAUD/DN_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg.csv\",sep=\";\",on_bad_lines='skip')\n",
    "anno7_df.columns = [\"Token_7\",\"Label_7\"]\n",
    "anno7_df=anno7_df.iloc[:6500]\n",
    "print(\"pendant anno7\",anno7_df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pendant anno1          Token_1 Label_1\n",
      "0             LA     NaN\n",
      "1       FABRIOUE     NaN\n",
      "2           \\n\\n     NaN\n",
      "3             EE     NaN\n",
      "4             \\n     NaN\n",
      "...          ...     ...\n",
      "6495          la     NaN\n",
      "6496  sentinelle     NaN\n",
      "6497      criait     NaN\n",
      "6498           :     NaN\n",
      "6499                 NaN\n",
      "\n",
      "[6500 rows x 2 columns]\n",
      "pendant anno2          Token_2 Label_2\n",
      "0             LA     NaN\n",
      "1       FABRIOUE     NaN\n",
      "2           \\n\\n     NaN\n",
      "3             EE     NaN\n",
      "4             \\n     NaN\n",
      "...          ...     ...\n",
      "6495          la     NaN\n",
      "6496  sentinelle     NaN\n",
      "6497      criait     NaN\n",
      "6498           :     478\n",
      "6499                 NaN\n",
      "\n",
      "[6500 rows x 2 columns]\n",
      "pendant anno3          Token_3 Label_3\n",
      "0             LA     NaN\n",
      "1       FABRIOUE     NaN\n",
      "2           \\n\\n     NaN\n",
      "3             EE     NaN\n",
      "4             \\n     NaN\n",
      "...          ...     ...\n",
      "6495          la     NaN\n",
      "6496  sentinelle     NaN\n",
      "6497      criait     NaN\n",
      "6498           :     NaN\n",
      "6499                 NaN\n",
      "\n",
      "[6500 rows x 2 columns]\n",
      "pendant anno4          Token_4 Label_4\n",
      "0             LA     NaN\n",
      "1       FABRIOUE     NaN\n",
      "2           \\n\\n     NaN\n",
      "3             EE     NaN\n",
      "4             \\n     NaN\n",
      "...          ...     ...\n",
      "6495          la     NaN\n",
      "6496  sentinelle     NaN\n",
      "6497      criait     NaN\n",
      "6498           :     NaN\n",
      "6499                 NaN\n",
      "\n",
      "[6500 rows x 2 columns]\n",
      "pendant anno6          Token_6 Label_6\n",
      "0             LA     LOC\n",
      "1       FABRIOUE     LOC\n",
      "2           \\n\\n     NaN\n",
      "3             EE     NaN\n",
      "4             \\n     NaN\n",
      "...          ...     ...\n",
      "6495          la     NaN\n",
      "6496  sentinelle     NaN\n",
      "6497      criait     NaN\n",
      "6498           :     NaN\n",
      "6499                 NaN\n",
      "\n",
      "[6500 rows x 2 columns]\n",
      "pendant anno7          Token_7 Label_7\n",
      "0             LA     NaN\n",
      "1       FABRIOUE     NaN\n",
      "2           \\n\\n     NaN\n",
      "3             EE     NaN\n",
      "4             \\n     NaN\n",
      "...          ...     ...\n",
      "6495          la     NaN\n",
      "6496  sentinelle     NaN\n",
      "6497      criait     NaN\n",
      "6498           :     NaN\n",
      "6499                 NaN\n",
      "\n",
      "[6500 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "id": "18bc07a0-bf32-4823-ae13-8e75fa156a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:56:26.319986Z",
     "start_time": "2024-10-26T07:56:26.302042Z"
    }
   },
   "source": [
    "# Join so we have a data frame of labels for each tweet ID\n",
    "# merged_df = anno7_df.join(anno6_df.join(anno5_df.join(anno4_df.join(anno1_df.join(anno2_df.join(anno3_df))))))\n",
    "merged_df = anno7_df.join(anno6_df.join(anno4_df.join(anno1_df.join(anno2_df.join(anno3_df)))))\n",
    "# merged_df = anno2_df.join(anno3_df)\n",
    "print(merged_df)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Token_7 Label_7     Token_6 Label_6     Token_4 Label_4     Token_1  \\\n",
      "0             LA     NaN          LA     LOC          LA     NaN          LA   \n",
      "1       FABRIOUE     NaN    FABRIOUE     LOC    FABRIOUE     NaN    FABRIOUE   \n",
      "2           \\n\\n     NaN        \\n\\n     NaN        \\n\\n     NaN        \\n\\n   \n",
      "3             EE     NaN          EE     NaN          EE     NaN          EE   \n",
      "4             \\n     NaN          \\n     NaN          \\n     NaN          \\n   \n",
      "...          ...     ...         ...     ...         ...     ...         ...   \n",
      "6495          la     NaN          la     NaN          la     NaN          la   \n",
      "6496  sentinelle     NaN  sentinelle     NaN  sentinelle     NaN  sentinelle   \n",
      "6497      criait     NaN      criait     NaN      criait     NaN      criait   \n",
      "6498           :     NaN           :     NaN           :     NaN           :   \n",
      "6499                 NaN                 NaN                 NaN               \n",
      "\n",
      "     Label_1     Token_2 Label_2     Token_3 Label_3  \n",
      "0        NaN          LA     NaN          LA     NaN  \n",
      "1        NaN    FABRIOUE     NaN    FABRIOUE     NaN  \n",
      "2        NaN        \\n\\n     NaN        \\n\\n     NaN  \n",
      "3        NaN          EE     NaN          EE     NaN  \n",
      "4        NaN          \\n     NaN          \\n     NaN  \n",
      "...      ...         ...     ...         ...     ...  \n",
      "6495     NaN          la     NaN          la     NaN  \n",
      "6496     NaN  sentinelle     NaN  sentinelle     NaN  \n",
      "6497     NaN      criait     NaN      criait     NaN  \n",
      "6498     NaN           :     478           :     NaN  \n",
      "6499     NaN                 NaN                 NaN  \n",
      "\n",
      "[6500 rows x 12 columns]\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:56:27.585518Z",
     "start_time": "2024-10-26T07:56:27.518217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can drop rows where we don't have two labelers if we want\n",
    "#. I think Krippendorf can account for missing data though, so this\n",
    "#. step isn't strictly necessary if you only care about Krippendorf\n",
    "labels_matched_df = merged_df.dropna()\n",
    "print(\"labels_matched_df\",labels_matched_df)\n",
    "labels_matched_df.to_csv(f'{auteur}_{version}_accord_interannot.csv', index=False)\n",
    "# Reformat the data into the form AnnotationTask\n",
    "#  expects.\n",
    "data = []\n",
    "for idx, row in labels_matched_df.iterrows():\n",
    "    data.append((\"a1\", idx, row[\"Label_1\"]))\n",
    "    data.append((\"a2\", idx, row[\"Label_2\"]))\n",
    "    data.append((\"a3\", idx, row[\"Label_3\"]))\n",
    "    data.append((\"a4\", idx, row[\"Label_4\"]))\n",
    "    # data.append((\"a5\", idx, row[\"Label_5\"]))\n",
    "    data.append((\"a6\", idx, row[\"Label_6\"]))\n",
    "    data.append((\"a7\", idx, row[\"Label_7\"]))\n",
    "# print(\"-----------------------------------------------------\",data)\n",
    "    \n",
    "atask = agreement.AnnotationTask(data=data)\n",
    "\n",
    "\n",
    "\n",
    "# This function maps labels into a numeric space,\n",
    "#. so we can rely on the ordering of labels.\n",
    "def priority_distance(left_label, right_label):\n",
    "    mapped_labels = {\n",
    "        \"Critical\": 4,\n",
    "        \"High\": 3,\n",
    "        \"Medium\": 2,\n",
    "        \"Low\": 1,\n",
    "    }\n",
    "    left_i = mapped_labels[left_label]\n",
    "    right_i = mapped_labels[right_label]\n",
    "    \n",
    "    return abs(left_i - right_i)\n",
    "\n",
    "# Create a new annotation task with the distance function\n",
    "# atask_2 = agreement.AnnotationTask(data=data, distance=priority_distance)\n",
    "atask_2 = agreement.AnnotationTask(data=data)\n",
    "\n",
    "print(\"Cohen's Kappa:\", atask.kappa())\n",
    "print(\"Fleiss's Kappa:\", atask.multi_kappa())\n",
    "print(\"Krippendorf's Alpha:\", atask_2.alpha())\n",
    "\n"
   ],
   "id": "be194fcbc8fb95ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_matched_df            Token_7 Label_7       Token_6 Label_6       Token_4 Label_4  \\\n",
      "18       Languedoc     LOC     Languedoc     LOC     Languedoc     LOC   \n",
      "32            Midi     LOC          Midi     LOC          Midi     LOC   \n",
      "47      Carmelites     LOC    Carmelites     LOC    Carmelites     ORG   \n",
      "61        Eyssette     PER      Eyssette     PER      Eyssette     PER   \n",
      "219       Eyssette     ORG      Eyssette     ORG      Eyssette     LOC   \n",
      "...            ...     ...           ...     ...           ...     ...   \n",
      "6191  Saint-Nizier     LOC  Saint-Nizier     LOC  Saint-Nizier     LOC   \n",
      "6207        Saint-     ORG        Saint-     LOC        Saint-     LOC   \n",
      "6209        Nizier     ORG        Nizier     LOC        Nizier     LOC   \n",
      "6330          abbe     PER          abbe     PER          abbe     PER   \n",
      "6331         Micou     PER         Micou     PER         Micou     PER   \n",
      "\n",
      "           Token_1 Label_1       Token_2 Label_2       Token_3 Label_3  \n",
      "18       Languedoc     LOC     Languedoc     LOC     Languedoc     LOC  \n",
      "32            Midi     LOC          Midi     LOC          Midi     LOC  \n",
      "47      Carmelites     LOC    Carmelites     LOC    Carmelites     LOC  \n",
      "61        Eyssette     PER      Eyssette     PER      Eyssette     PER  \n",
      "219       Eyssette     LOC      Eyssette     LOC      Eyssette     LOC  \n",
      "...            ...     ...           ...     ...           ...     ...  \n",
      "6191  Saint-Nizier     LOC  Saint-Nizier     LOC  Saint-Nizier     LOC  \n",
      "6207        Saint-     LOC        Saint-     LOC        Saint-     LOC  \n",
      "6209        Nizier     LOC        Nizier     LOC        Nizier     LOC  \n",
      "6330          abbe     PER          abbe     PER          abbe     PER  \n",
      "6331         Micou     PER         Micou     PER         Micou     PER  \n",
      "\n",
      "[144 rows x 12 columns]\n",
      "Cohen's Kappa: 0.723587629381191\n",
      "Fleiss's Kappa: 0.7134283096945383\n",
      "Krippendorf's Alpha: 0.7128006021948416\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T07:56:29.268433Z",
     "start_time": "2024-10-26T07:56:29.211778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "def stocker( chemin, contenu):\n",
    "    #if os.path.exists(chemin)==True:  # Où mettre la sécurité ?\n",
    "        #print(f\"Already Done {chemin}\")\n",
    "    w =open(chemin, \"w\")\n",
    "    w.write(json.dumps(contenu , indent = 2))\n",
    "    w.close()\n",
    "    \n",
    "dico_resultat = {}\n",
    "dico_resultat[\"Cohen's Kappa\"] = atask.kappa()\n",
    "dico_resultat[\"Fleiss's Kappa\"] = atask.multi_kappa()\n",
    "dico_resultat[\"Krippendorf's Alpha\"] = atask_2.alpha()\n",
    "stocker(f\"{auteur}_{version}_IAA.json\",dico_resultat)"
   ],
   "id": "fa2001fcff08c602",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "43059d85f4be35f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
